{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d05462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4122cbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>111.61</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>111.78</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>110.19</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>110.74</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>111.58</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  114.70  10.605   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  114.72  10.598   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  114.71  10.601   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  114.72  10.606   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  114.72  10.612   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  111.61  10.400   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  111.78  10.433   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  110.19  10.483   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  110.74  10.533   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  111.58  10.583   \n",
       "\n",
       "           CO     NOX  \n",
       "0      3.1547  82.722  \n",
       "1      3.2363  82.776  \n",
       "2      3.2012  82.468  \n",
       "3      3.1923  82.670  \n",
       "4      3.2484  82.311  \n",
       "...       ...     ...  \n",
       "15034  4.5186  79.559  \n",
       "15035  4.8470  79.917  \n",
       "15036  7.9632  90.912  \n",
       "15037  6.2494  93.227  \n",
       "15038  4.9816  92.498  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv(\"gas_turbines.csv\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2949f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>546.21</td>\n",
       "      <td>111.61</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>543.22</td>\n",
       "      <td>111.78</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>537.32</td>\n",
       "      <td>110.19</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>541.24</td>\n",
       "      <td>110.74</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>545.85</td>\n",
       "      <td>111.58</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TAT     TEY      CO     NOX\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  550.00  114.70  3.1547  82.722\n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  550.00  114.72  3.2363  82.776\n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  549.87  114.71  3.2012  82.468\n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  549.99  114.72  3.1923  82.670\n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  549.98  114.72  3.2484  82.311\n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...\n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  546.21  111.61  4.5186  79.559\n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  543.22  111.78  4.8470  79.917\n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  537.32  110.19  7.9632  90.912\n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  541.24  110.74  6.2494  93.227\n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  545.85  111.58  4.9816  92.498\n",
       "\n",
       "[15039 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.drop(['TIT','CDP'], axis=1, inplace=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17c0b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AT      float64\n",
      "AP      float64\n",
      "AH      float64\n",
      "AFDP    float64\n",
      "GTEP    float64\n",
      "TAT     float64\n",
      "TEY     float64\n",
      "CO      float64\n",
      "NOX     float64\n",
      "dtype: object\n",
      "AT      0\n",
      "AP      0\n",
      "AH      0\n",
      "AFDP    0\n",
      "GTEP    0\n",
      "TAT     0\n",
      "TEY     0\n",
      "CO      0\n",
      "NOX     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.00000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17.764381</td>\n",
       "      <td>1013.19924</td>\n",
       "      <td>79.124174</td>\n",
       "      <td>4.200294</td>\n",
       "      <td>25.419061</td>\n",
       "      <td>545.396183</td>\n",
       "      <td>134.188464</td>\n",
       "      <td>1.972499</td>\n",
       "      <td>68.190934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.574323</td>\n",
       "      <td>6.41076</td>\n",
       "      <td>13.793439</td>\n",
       "      <td>0.760197</td>\n",
       "      <td>4.173916</td>\n",
       "      <td>7.866803</td>\n",
       "      <td>15.829717</td>\n",
       "      <td>2.222206</td>\n",
       "      <td>10.470586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.522300</td>\n",
       "      <td>985.85000</td>\n",
       "      <td>30.344000</td>\n",
       "      <td>2.087400</td>\n",
       "      <td>17.878000</td>\n",
       "      <td>512.450000</td>\n",
       "      <td>100.170000</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>27.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.408000</td>\n",
       "      <td>1008.90000</td>\n",
       "      <td>69.750000</td>\n",
       "      <td>3.723900</td>\n",
       "      <td>23.294000</td>\n",
       "      <td>542.170000</td>\n",
       "      <td>127.985000</td>\n",
       "      <td>0.858055</td>\n",
       "      <td>61.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.186000</td>\n",
       "      <td>1012.80000</td>\n",
       "      <td>82.266000</td>\n",
       "      <td>4.186200</td>\n",
       "      <td>25.082000</td>\n",
       "      <td>549.890000</td>\n",
       "      <td>133.780000</td>\n",
       "      <td>1.390200</td>\n",
       "      <td>66.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23.862500</td>\n",
       "      <td>1016.90000</td>\n",
       "      <td>90.043500</td>\n",
       "      <td>4.550900</td>\n",
       "      <td>27.184000</td>\n",
       "      <td>550.060000</td>\n",
       "      <td>140.895000</td>\n",
       "      <td>2.160400</td>\n",
       "      <td>73.935500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>34.929000</td>\n",
       "      <td>1034.20000</td>\n",
       "      <td>100.200000</td>\n",
       "      <td>7.610600</td>\n",
       "      <td>37.402000</td>\n",
       "      <td>550.610000</td>\n",
       "      <td>174.610000</td>\n",
       "      <td>44.103000</td>\n",
       "      <td>119.890000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AT           AP            AH          AFDP          GTEP  \\\n",
       "count  15039.000000  15039.00000  15039.000000  15039.000000  15039.000000   \n",
       "mean      17.764381   1013.19924     79.124174      4.200294     25.419061   \n",
       "std        7.574323      6.41076     13.793439      0.760197      4.173916   \n",
       "min        0.522300    985.85000     30.344000      2.087400     17.878000   \n",
       "25%       11.408000   1008.90000     69.750000      3.723900     23.294000   \n",
       "50%       18.186000   1012.80000     82.266000      4.186200     25.082000   \n",
       "75%       23.862500   1016.90000     90.043500      4.550900     27.184000   \n",
       "max       34.929000   1034.20000    100.200000      7.610600     37.402000   \n",
       "\n",
       "                TAT           TEY            CO           NOX  \n",
       "count  15039.000000  15039.000000  15039.000000  15039.000000  \n",
       "mean     545.396183    134.188464      1.972499     68.190934  \n",
       "std        7.866803     15.829717      2.222206     10.470586  \n",
       "min      512.450000    100.170000      0.000388     27.765000  \n",
       "25%      542.170000    127.985000      0.858055     61.303500  \n",
       "50%      549.890000    133.780000      1.390200     66.601000  \n",
       "75%      550.060000    140.895000      2.160400     73.935500  \n",
       "max      550.610000    174.610000     44.103000    119.890000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ds.dtypes)\n",
    "print(ds.isnull().sum())\n",
    "ds.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ddab2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AT</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.412953</td>\n",
       "      <td>-0.549432</td>\n",
       "      <td>-0.099333</td>\n",
       "      <td>-0.049103</td>\n",
       "      <td>0.338569</td>\n",
       "      <td>-0.207495</td>\n",
       "      <td>-0.088588</td>\n",
       "      <td>-0.600006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>-0.412953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.042573</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>-0.223479</td>\n",
       "      <td>0.146939</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.256744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AH</th>\n",
       "      <td>-0.549432</td>\n",
       "      <td>0.042573</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.119249</td>\n",
       "      <td>-0.202784</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>-0.110272</td>\n",
       "      <td>0.165505</td>\n",
       "      <td>0.143061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFDP</th>\n",
       "      <td>-0.099333</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>-0.119249</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.744251</td>\n",
       "      <td>-0.571541</td>\n",
       "      <td>0.717995</td>\n",
       "      <td>-0.334207</td>\n",
       "      <td>-0.037299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GTEP</th>\n",
       "      <td>-0.049103</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>-0.202784</td>\n",
       "      <td>0.744251</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.756884</td>\n",
       "      <td>0.977042</td>\n",
       "      <td>-0.508259</td>\n",
       "      <td>-0.208496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAT</th>\n",
       "      <td>0.338569</td>\n",
       "      <td>-0.223479</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>-0.571541</td>\n",
       "      <td>-0.756884</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.720356</td>\n",
       "      <td>0.063404</td>\n",
       "      <td>0.009888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEY</th>\n",
       "      <td>-0.207495</td>\n",
       "      <td>0.146939</td>\n",
       "      <td>-0.110272</td>\n",
       "      <td>0.717995</td>\n",
       "      <td>0.977042</td>\n",
       "      <td>-0.720356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.541751</td>\n",
       "      <td>-0.102631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CO</th>\n",
       "      <td>-0.088588</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.165505</td>\n",
       "      <td>-0.334207</td>\n",
       "      <td>-0.508259</td>\n",
       "      <td>0.063404</td>\n",
       "      <td>-0.541751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>-0.600006</td>\n",
       "      <td>0.256744</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>-0.037299</td>\n",
       "      <td>-0.208496</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>-0.102631</td>\n",
       "      <td>0.316743</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AT        AP        AH      AFDP      GTEP       TAT       TEY  \\\n",
       "AT    1.000000 -0.412953 -0.549432 -0.099333 -0.049103  0.338569 -0.207495   \n",
       "AP   -0.412953  1.000000  0.042573  0.040318  0.078575 -0.223479  0.146939   \n",
       "AH   -0.549432  0.042573  1.000000 -0.119249 -0.202784  0.010859 -0.110272   \n",
       "AFDP -0.099333  0.040318 -0.119249  1.000000  0.744251 -0.571541  0.717995   \n",
       "GTEP -0.049103  0.078575 -0.202784  0.744251  1.000000 -0.756884  0.977042   \n",
       "TAT   0.338569 -0.223479  0.010859 -0.571541 -0.756884  1.000000 -0.720356   \n",
       "TEY  -0.207495  0.146939 -0.110272  0.717995  0.977042 -0.720356  1.000000   \n",
       "CO   -0.088588  0.041614  0.165505 -0.334207 -0.508259  0.063404 -0.541751   \n",
       "NOX  -0.600006  0.256744  0.143061 -0.037299 -0.208496  0.009888 -0.102631   \n",
       "\n",
       "            CO       NOX  \n",
       "AT   -0.088588 -0.600006  \n",
       "AP    0.041614  0.256744  \n",
       "AH    0.165505  0.143061  \n",
       "AFDP -0.334207 -0.037299  \n",
       "GTEP -0.508259 -0.208496  \n",
       "TAT   0.063404  0.009888  \n",
       "TEY  -0.541751 -0.102631  \n",
       "CO    1.000000  0.316743  \n",
       "NOX   0.316743  1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347450dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TEY      AT      AP      AH    AFDP    GTEP     TAT      CO     NOX\n",
      "0  114.70  6.8594  1007.9  96.799  3.5000  19.663  550.00  3.1547  82.722\n",
      "1  114.72  6.7850  1008.4  97.118  3.4998  19.728  550.00  3.2363  82.776\n",
      "2  114.71  6.8977  1008.8  95.939  3.4824  19.779  549.87  3.2012  82.468\n",
      "3  114.72  7.0569  1009.2  95.249  3.4805  19.792  549.99  3.1923  82.670\n",
      "4  114.72  7.3978  1009.7  95.150  3.4976  19.765  549.98  3.2484  82.311\n"
     ]
    }
   ],
   "source": [
    "# moving the TEY column to the 0th position in the table\n",
    "lastCol = ds.pop('TEY')\n",
    "ds.insert(0 , 'TEY', lastCol)\n",
    "print(ds.head(5))\n",
    "\n",
    "#assigning predictor variables to x and response variable to y\n",
    "x = ds.iloc[:,1:]\n",
    "y = ds[['TEY']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0b3093",
   "metadata": {},
   "source": [
    "### Standardizing only predictor variable - after train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9806ef16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12031, 8)\n",
      "(3008, 8)\n",
      "(12031, 1)\n",
      "(3008, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.20, random_state=42)\n",
    "\n",
    "scaler_train = StandardScaler()\n",
    "scaler_test = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler_train.fit_transform(x_train) # scaling train data -- predictor\n",
    "x_test_scaled  = scaler_test.fit_transform(x_test) # scaling test data -- predictor\n",
    "\n",
    "print(x_train_scaled.shape)\n",
    "print(x_test_scaled.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "#for removing heading from y_test\n",
    "y_test = y_test.values\n",
    "#print(x_train_scaled)\n",
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd3d5080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 625us/step - MeanSquaredError: 17767.9355 - loss: 17767.9062\n",
      "Epoch 2/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - MeanSquaredError: 15937.9854 - loss: 15937.9844\n",
      "Epoch 3/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - MeanSquaredError: 14685.9805 - loss: 14685.9688\n",
      "Epoch 4/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - MeanSquaredError: 13453.0908 - loss: 13453.0918\n",
      "Epoch 5/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - MeanSquaredError: 12417.0625 - loss: 12417.0586\n",
      "Epoch 6/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - MeanSquaredError: 11381.0293 - loss: 11381.0293\n",
      "Epoch 7/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - MeanSquaredError: 10410.9502 - loss: 10410.9434\n",
      "Epoch 8/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - MeanSquaredError: 9530.3164 - loss: 9530.3057\n",
      "Epoch 9/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - MeanSquaredError: 8681.6279 - loss: 8681.6260\n",
      "Epoch 10/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - MeanSquaredError: 7849.0278 - loss: 7849.0327\n",
      "Epoch 11/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step - MeanSquaredError: 7108.3472 - loss: 7108.3398\n",
      "Epoch 12/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - MeanSquaredError: 6400.2764 - loss: 6400.2656\n",
      "Epoch 13/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - MeanSquaredError: 5799.9517 - loss: 5799.9619\n",
      "Epoch 14/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - MeanSquaredError: 5226.7134 - loss: 5226.7139\n",
      "Epoch 15/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - MeanSquaredError: 4638.5674 - loss: 4638.5547\n",
      "Epoch 16/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - MeanSquaredError: 4114.9492 - loss: 4114.9458\n",
      "Epoch 17/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - MeanSquaredError: 3667.3081 - loss: 3667.3052\n",
      "Epoch 18/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - MeanSquaredError: 3220.7075 - loss: 3220.7004\n",
      "Epoch 19/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - MeanSquaredError: 2787.4514 - loss: 2787.4485\n",
      "Epoch 20/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - MeanSquaredError: 2416.6018 - loss: 2416.6025\n",
      "Epoch 21/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - MeanSquaredError: 2123.3057 - loss: 2123.3032\n",
      "Epoch 22/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - MeanSquaredError: 1824.4542 - loss: 1824.4524\n",
      "Epoch 23/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - MeanSquaredError: 1541.3735 - loss: 1541.3691\n",
      "Epoch 24/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - MeanSquaredError: 1321.5085 - loss: 1321.5072\n",
      "Epoch 25/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - MeanSquaredError: 1116.4741 - loss: 1116.4688\n",
      "Epoch 26/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - MeanSquaredError: 940.2386 - loss: 940.2357\n",
      "Epoch 27/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - MeanSquaredError: 795.0996 - loss: 795.0995\n",
      "Epoch 28/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - MeanSquaredError: 672.1407 - loss: 672.1394\n",
      "Epoch 29/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - MeanSquaredError: 565.0129 - loss: 565.0092\n",
      "Epoch 30/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - MeanSquaredError: 471.2467 - loss: 471.2454\n",
      "Epoch 31/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - MeanSquaredError: 413.5404 - loss: 413.5400\n",
      "Epoch 32/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - MeanSquaredError: 367.1783 - loss: 367.1802\n",
      "Epoch 33/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - MeanSquaredError: 329.6370 - loss: 329.6366\n",
      "Epoch 34/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - MeanSquaredError: 303.3434 - loss: 303.3417\n",
      "Epoch 35/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - MeanSquaredError: 275.8546 - loss: 275.8556\n",
      "Epoch 36/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - MeanSquaredError: 263.3940 - loss: 263.3941\n",
      "Epoch 37/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - MeanSquaredError: 254.1143 - loss: 254.1142\n",
      "Epoch 38/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - MeanSquaredError: 258.1825 - loss: 258.1822\n",
      "Epoch 39/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - MeanSquaredError: 253.0617 - loss: 253.0616\n",
      "Epoch 40/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step - MeanSquaredError: 209.2742 - loss: 209.2742\n",
      "Epoch 41/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - MeanSquaredError: 115.4639 - loss: 115.4634\n",
      "Epoch 42/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step - MeanSquaredError: 90.2442 - loss: 90.2449\n",
      "Epoch 43/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - MeanSquaredError: 70.9519 - loss: 70.9524\n",
      "Epoch 44/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - MeanSquaredError: 54.9964 - loss: 54.9965\n",
      "Epoch 45/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - MeanSquaredError: 40.3607 - loss: 40.3608\n",
      "Epoch 46/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - MeanSquaredError: 32.3104 - loss: 32.3103\n",
      "Epoch 47/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - MeanSquaredError: 25.2310 - loss: 25.2310\n",
      "Epoch 48/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - MeanSquaredError: 18.9443 - loss: 18.9442\n",
      "Epoch 49/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - MeanSquaredError: 15.2005 - loss: 15.2005\n",
      "Epoch 50/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - MeanSquaredError: 10.9595 - loss: 10.9594\n",
      "Epoch 51/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - MeanSquaredError: 8.4139 - loss: 8.4138\n",
      "Epoch 52/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - MeanSquaredError: 6.3475 - loss: 6.3474\n",
      "Epoch 53/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - MeanSquaredError: 5.0493 - loss: 5.0493\n",
      "Epoch 54/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - MeanSquaredError: 3.9358 - loss: 3.9358\n",
      "Epoch 55/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - MeanSquaredError: 3.1471 - loss: 3.1471\n",
      "Epoch 56/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - MeanSquaredError: 2.5859 - loss: 2.5858\n",
      "Epoch 57/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - MeanSquaredError: 2.2046 - loss: 2.2046\n",
      "Epoch 58/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - MeanSquaredError: 1.7221 - loss: 1.7221\n",
      "Epoch 59/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - MeanSquaredError: 1.5777 - loss: 1.5777\n",
      "Epoch 60/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - MeanSquaredError: 1.4132 - loss: 1.4132\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - MeanSquaredError: 1.3483 - loss: 1.3483\n",
      "Epoch 62/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - MeanSquaredError: 1.1700 - loss: 1.1700\n",
      "Epoch 63/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - MeanSquaredError: 1.0088 - loss: 1.0088\n",
      "Epoch 64/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - MeanSquaredError: 1.0157 - loss: 1.0158\n",
      "Epoch 65/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - MeanSquaredError: 0.8983 - loss: 0.8983\n",
      "Epoch 66/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - MeanSquaredError: 0.8916 - loss: 0.8916\n",
      "Epoch 67/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step - MeanSquaredError: 0.8631 - loss: 0.8631\n",
      "Epoch 68/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - MeanSquaredError: 0.8252 - loss: 0.8252\n",
      "Epoch 69/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606us/step - MeanSquaredError: 0.7423 - loss: 0.7423\n",
      "Epoch 70/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - MeanSquaredError: 0.7200 - loss: 0.7200\n",
      "Epoch 71/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - MeanSquaredError: 0.7064 - loss: 0.7064\n",
      "Epoch 72/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step - MeanSquaredError: 0.7000 - loss: 0.7000\n",
      "Epoch 73/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - MeanSquaredError: 0.6856 - loss: 0.6856\n",
      "Epoch 74/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - MeanSquaredError: 0.6786 - loss: 0.6786\n",
      "Epoch 75/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - MeanSquaredError: 0.6548 - loss: 0.6548\n",
      "Epoch 76/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - MeanSquaredError: 0.6381 - loss: 0.6381\n",
      "Epoch 77/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - MeanSquaredError: 0.6250 - loss: 0.6250\n",
      "Epoch 78/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - MeanSquaredError: 0.6466 - loss: 0.6466\n",
      "Epoch 79/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - MeanSquaredError: 0.6059 - loss: 0.6059\n",
      "Epoch 80/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - MeanSquaredError: 0.6084 - loss: 0.6084\n",
      "Epoch 81/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - MeanSquaredError: 0.5782 - loss: 0.5782\n",
      "Epoch 82/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - MeanSquaredError: 0.5738 - loss: 0.5738\n",
      "Epoch 83/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step - MeanSquaredError: 0.5796 - loss: 0.5796\n",
      "Epoch 84/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - MeanSquaredError: 0.5743 - loss: 0.5743\n",
      "Epoch 85/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - MeanSquaredError: 0.6512 - loss: 0.6512\n",
      "Epoch 86/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - MeanSquaredError: 0.5594 - loss: 0.5594\n",
      "Epoch 87/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - MeanSquaredError: 0.5731 - loss: 0.5731\n",
      "Epoch 88/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636us/step - MeanSquaredError: 0.5487 - loss: 0.5487\n",
      "Epoch 89/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - MeanSquaredError: 0.5948 - loss: 0.5948\n",
      "Epoch 90/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - MeanSquaredError: 0.5721 - loss: 0.5721\n",
      "Epoch 91/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step - MeanSquaredError: 0.5469 - loss: 0.5469\n",
      "Epoch 92/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - MeanSquaredError: 0.6107 - loss: 0.6107\n",
      "Epoch 93/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - MeanSquaredError: 0.5475 - loss: 0.5475\n",
      "Epoch 94/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - MeanSquaredError: 0.5514 - loss: 0.5514\n",
      "Epoch 95/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - MeanSquaredError: 0.5589 - loss: 0.5589\n",
      "Epoch 96/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - MeanSquaredError: 0.5336 - loss: 0.5336\n",
      "Epoch 97/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step - MeanSquaredError: 0.5629 - loss: 0.5629\n",
      "Epoch 98/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - MeanSquaredError: 0.5412 - loss: 0.5412\n",
      "Epoch 99/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - MeanSquaredError: 0.5516 - loss: 0.5516\n",
      "Epoch 100/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - MeanSquaredError: 0.5871 - loss: 0.5871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26e09496b90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we have continuous ouput, AF is not required in the o/p layer\n",
    "model = Sequential()\n",
    "model.add( Dense( units = 50 , activation = 'relu' , kernel_initializer = 'normal', input_dim = 8)) # input layer\n",
    "model.add( Dense( units = 20 , activation = 'tanh' , kernel_initializer = 'normal' )) # hidden layer\n",
    "model.add( Dense( units = 1  , kernel_initializer = 'normal' )) # o/p layer\n",
    "\n",
    "model.compile(optimizer ='adam', loss = 'mean_squared_error', metrics=['MeanSquaredError'])\n",
    "model.fit(x_train_scaled, y_train , batch_size=50, epochs=100,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b8660b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step\n",
      "batch_size: 5 - epochs: 5 Accuracy: 98.52522309519256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506us/step\n",
      "batch_size: 5 - epochs: 10 Accuracy: 99.47992482483473\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step\n",
      "batch_size: 5 - epochs: 50 Accuracy: 99.58955211438598\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step\n",
      "batch_size: 5 - epochs: 100 Accuracy: 99.6082930153929\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step\n",
      "batch_size: 10 - epochs: 5 Accuracy: 81.12507347584375\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step\n",
      "batch_size: 10 - epochs: 10 Accuracy: 98.7766093359925\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step\n",
      "batch_size: 10 - epochs: 50 Accuracy: 99.5345198251479\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "batch_size: 10 - epochs: 100 Accuracy: 99.5623200272616\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "batch_size: 15 - epochs: 5 Accuracy: 58.92387268601717\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step\n",
      "batch_size: 15 - epochs: 10 Accuracy: 89.85945837953977\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step\n",
      "batch_size: 15 - epochs: 50 Accuracy: 99.5373677294856\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step\n",
      "batch_size: 15 - epochs: 100 Accuracy: 99.53288930670772\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step\n",
      "batch_size: 20 - epochs: 5 Accuracy: 46.03637627521483\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step\n",
      "batch_size: 20 - epochs: 10 Accuracy: 80.97841335566565\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "batch_size: 20 - epochs: 50 Accuracy: 99.5232587957692\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step\n",
      "batch_size: 20 - epochs: 100 Accuracy: 99.56368611035626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batchsize</th>\n",
       "      <th>epochs</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>98.525223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>99.479925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>99.589552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>99.608293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>81.125073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>98.776609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>99.534520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>99.562320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>58.923873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>89.859458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>50</td>\n",
       "      <td>99.537368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>99.532889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>46.036376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>80.978413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>99.523259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>99.563686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batchsize  epochs   Accuracy\n",
       "0          5       5  98.525223\n",
       "0          5      10  99.479925\n",
       "0          5      50  99.589552\n",
       "0          5     100  99.608293\n",
       "0         10       5  81.125073\n",
       "0         10      10  98.776609\n",
       "0         10      50  99.534520\n",
       "0         10     100  99.562320\n",
       "0         15       5  58.923873\n",
       "0         15      10  89.859458\n",
       "0         15      50  99.537368\n",
       "0         15     100  99.532889\n",
       "0         20       5  46.036376\n",
       "0         20      10  80.978413\n",
       "0         20      50  99.523259\n",
       "0         20     100  99.563686"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def toFindBestParams(x_train_scaled, y_train, x_test_scaled, y_test):\n",
    "        \n",
    "    #defining list of hyperparameters\n",
    "    batch_size_list = [5 , 10 , 15 , 20]\n",
    "    epoch_list      = [5 , 10 , 50 , 100]\n",
    "     \n",
    "    bestParamTable = pd.DataFrame()\n",
    "    \n",
    "    for batch_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "            \n",
    "            # create ANN model\n",
    "            model = Sequential()\n",
    "            # Defining the first layer of the model\n",
    "            model.add(Dense(units=50, input_dim=x_train_scaled.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "            \n",
    "            # Defining the Second layer of the model\n",
    "            model.add(Dense(units=20, kernel_initializer='normal', activation='tanh'))\n",
    " \n",
    "            # The output neuron is a single fully connected node \n",
    "            # Since we will be predicting a single number\n",
    "            model.add(Dense(1, kernel_initializer='normal'))\n",
    " \n",
    "            # Compiling the model\n",
    "            model.compile(optimizer ='adam', loss = 'mean_squared_error')\n",
    "            \n",
    "            # Fitting the ANN to the Training set\n",
    "            model.fit(x_train_scaled, y_train , batch_size=batch_trial, epochs=epochs_trial,  verbose=0)\n",
    "                        \n",
    "            MAPE = np.mean(100 * (np.abs(y_test-model.predict(x_test_scaled))/y_test))  \n",
    "                        \n",
    "            bestParamTable=bestParamTable.append(pd.DataFrame(data=[[batch_trial, epochs_trial, 100-MAPE]],\n",
    "                                                        columns=['batchsize','epochs','Accuracy'] ))\n",
    "            \n",
    "            # printing the results of the current iteration\n",
    "            print('batch_size:', batch_trial,'-', 'epochs:',epochs_trial, 'Accuracy:',100-MAPE)\n",
    "\n",
    "    return bestParamTable\n",
    "\n",
    "# Calling the function\n",
    "finalParamTable_1 = toFindBestParams(x_train_scaled, y_train, x_test_scaled, y_test)\n",
    "finalParamTable_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f021d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index          0.000000\n",
       "batchsize      5.000000\n",
       "epochs       100.000000\n",
       "Accuracy      99.608293\n",
       "Name: 3, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting corresponding row values of the maximum value of 'Accuracy' column\n",
    "finalParamTable_1 = finalParamTable_1.reset_index()\n",
    "#print(finalParamTable_1)\n",
    "#print(finalParamTable['Accuracy'].idxmax())\n",
    "finalParamTable_1.iloc[finalParamTable_1['Accuracy'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f83543d",
   "metadata": {},
   "source": [
    "## Training Model - using best params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e5e833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26e093b2d10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer ='adam', loss = 'mean_squared_error')\n",
    "# fitting the model to best params\n",
    "model.fit(x_train_scaled,y_train, batch_size=20 , epochs = 100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8444ff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step\n",
      "(3008, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TAT</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "      <th>Price</th>\n",
       "      <th>Predicted Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>24.0930</td>\n",
       "      <td>1010.2</td>\n",
       "      <td>75.821</td>\n",
       "      <td>4.0023</td>\n",
       "      <td>25.762</td>\n",
       "      <td>550.25</td>\n",
       "      <td>1.26430</td>\n",
       "      <td>60.954</td>\n",
       "      <td>134.46</td>\n",
       "      <td>134.573898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12627</th>\n",
       "      <td>20.4500</td>\n",
       "      <td>1014.4</td>\n",
       "      <td>90.471</td>\n",
       "      <td>3.2106</td>\n",
       "      <td>20.085</td>\n",
       "      <td>549.94</td>\n",
       "      <td>2.69370</td>\n",
       "      <td>56.658</td>\n",
       "      <td>111.88</td>\n",
       "      <td>112.686584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6393</th>\n",
       "      <td>20.2620</td>\n",
       "      <td>1012.5</td>\n",
       "      <td>82.892</td>\n",
       "      <td>4.5325</td>\n",
       "      <td>25.221</td>\n",
       "      <td>549.62</td>\n",
       "      <td>1.96250</td>\n",
       "      <td>64.937</td>\n",
       "      <td>133.72</td>\n",
       "      <td>134.042786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>26.8620</td>\n",
       "      <td>1012.3</td>\n",
       "      <td>70.267</td>\n",
       "      <td>4.4266</td>\n",
       "      <td>25.965</td>\n",
       "      <td>549.96</td>\n",
       "      <td>1.57120</td>\n",
       "      <td>64.836</td>\n",
       "      <td>133.79</td>\n",
       "      <td>134.094818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12462</th>\n",
       "      <td>19.4090</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>90.525</td>\n",
       "      <td>3.1241</td>\n",
       "      <td>19.688</td>\n",
       "      <td>550.01</td>\n",
       "      <td>2.29960</td>\n",
       "      <td>58.706</td>\n",
       "      <td>110.77</td>\n",
       "      <td>111.484459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7405</th>\n",
       "      <td>2.2158</td>\n",
       "      <td>1013.1</td>\n",
       "      <td>88.695</td>\n",
       "      <td>3.3709</td>\n",
       "      <td>19.387</td>\n",
       "      <td>550.17</td>\n",
       "      <td>4.27640</td>\n",
       "      <td>99.759</td>\n",
       "      <td>113.32</td>\n",
       "      <td>113.738152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10993</th>\n",
       "      <td>23.8520</td>\n",
       "      <td>1002.2</td>\n",
       "      <td>84.186</td>\n",
       "      <td>3.7871</td>\n",
       "      <td>25.392</td>\n",
       "      <td>550.11</td>\n",
       "      <td>0.83578</td>\n",
       "      <td>59.426</td>\n",
       "      <td>133.77</td>\n",
       "      <td>133.781769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9488</th>\n",
       "      <td>12.3950</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>95.381</td>\n",
       "      <td>4.2837</td>\n",
       "      <td>23.225</td>\n",
       "      <td>549.60</td>\n",
       "      <td>2.01980</td>\n",
       "      <td>75.260</td>\n",
       "      <td>128.98</td>\n",
       "      <td>129.835068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14201</th>\n",
       "      <td>12.3590</td>\n",
       "      <td>1022.7</td>\n",
       "      <td>82.295</td>\n",
       "      <td>5.1559</td>\n",
       "      <td>32.518</td>\n",
       "      <td>528.98</td>\n",
       "      <td>0.87760</td>\n",
       "      <td>66.416</td>\n",
       "      <td>159.42</td>\n",
       "      <td>160.687744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9757</th>\n",
       "      <td>8.6376</td>\n",
       "      <td>1001.6</td>\n",
       "      <td>98.271</td>\n",
       "      <td>5.9309</td>\n",
       "      <td>32.105</td>\n",
       "      <td>530.69</td>\n",
       "      <td>10.75000</td>\n",
       "      <td>102.130</td>\n",
       "      <td>161.86</td>\n",
       "      <td>161.905365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AT      AP      AH    AFDP    GTEP     TAT        CO      NOX  \\\n",
       "13312  24.0930  1010.2  75.821  4.0023  25.762  550.25   1.26430   60.954   \n",
       "12627  20.4500  1014.4  90.471  3.2106  20.085  549.94   2.69370   56.658   \n",
       "6393   20.2620  1012.5  82.892  4.5325  25.221  549.62   1.96250   64.937   \n",
       "4990   26.8620  1012.3  70.267  4.4266  25.965  549.96   1.57120   64.836   \n",
       "12462  19.4090  1006.3  90.525  3.1241  19.688  550.01   2.29960   58.706   \n",
       "7405    2.2158  1013.1  88.695  3.3709  19.387  550.17   4.27640   99.759   \n",
       "10993  23.8520  1002.2  84.186  3.7871  25.392  550.11   0.83578   59.426   \n",
       "9488   12.3950  1019.5  95.381  4.2837  23.225  549.60   2.01980   75.260   \n",
       "14201  12.3590  1022.7  82.295  5.1559  32.518  528.98   0.87760   66.416   \n",
       "9757    8.6376  1001.6  98.271  5.9309  32.105  530.69  10.75000  102.130   \n",
       "\n",
       "        Price  Predicted Price  \n",
       "13312  134.46       134.573898  \n",
       "12627  111.88       112.686584  \n",
       "6393   133.72       134.042786  \n",
       "4990   133.79       134.094818  \n",
       "12462  110.77       111.484459  \n",
       "7405   113.32       113.738152  \n",
       "10993  133.77       133.781769  \n",
       "9488   128.98       129.835068  \n",
       "14201  159.42       160.687744  \n",
       "9757   161.86       161.905365  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating predictions for test data\n",
    "y_predict_test = model.predict(x_test_scaled) \n",
    "\n",
    "# creating table with test price & predicted price for test\n",
    "final_table = pd.DataFrame(x_test)\n",
    "final_table['Price'] = y_test\n",
    "final_table['Predicted Price'] = y_predict_test\n",
    "print(final_table.shape)\n",
    "final_table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a1defe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy for Test Data -- ANN model =  99.6107855940776\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TAT</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "      <th>Price</th>\n",
       "      <th>Predicted Price</th>\n",
       "      <th>APE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>24.093</td>\n",
       "      <td>1010.2</td>\n",
       "      <td>75.821</td>\n",
       "      <td>4.0023</td>\n",
       "      <td>25.762</td>\n",
       "      <td>550.25</td>\n",
       "      <td>1.2643</td>\n",
       "      <td>60.954</td>\n",
       "      <td>134.46</td>\n",
       "      <td>134.573898</td>\n",
       "      <td>0.084708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12627</th>\n",
       "      <td>20.450</td>\n",
       "      <td>1014.4</td>\n",
       "      <td>90.471</td>\n",
       "      <td>3.2106</td>\n",
       "      <td>20.085</td>\n",
       "      <td>549.94</td>\n",
       "      <td>2.6937</td>\n",
       "      <td>56.658</td>\n",
       "      <td>111.88</td>\n",
       "      <td>112.686584</td>\n",
       "      <td>0.720937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6393</th>\n",
       "      <td>20.262</td>\n",
       "      <td>1012.5</td>\n",
       "      <td>82.892</td>\n",
       "      <td>4.5325</td>\n",
       "      <td>25.221</td>\n",
       "      <td>549.62</td>\n",
       "      <td>1.9625</td>\n",
       "      <td>64.937</td>\n",
       "      <td>133.72</td>\n",
       "      <td>134.042786</td>\n",
       "      <td>0.241389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>26.862</td>\n",
       "      <td>1012.3</td>\n",
       "      <td>70.267</td>\n",
       "      <td>4.4266</td>\n",
       "      <td>25.965</td>\n",
       "      <td>549.96</td>\n",
       "      <td>1.5712</td>\n",
       "      <td>64.836</td>\n",
       "      <td>133.79</td>\n",
       "      <td>134.094818</td>\n",
       "      <td>0.227833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12462</th>\n",
       "      <td>19.409</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>90.525</td>\n",
       "      <td>3.1241</td>\n",
       "      <td>19.688</td>\n",
       "      <td>550.01</td>\n",
       "      <td>2.2996</td>\n",
       "      <td>58.706</td>\n",
       "      <td>110.77</td>\n",
       "      <td>111.484459</td>\n",
       "      <td>0.644993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TAT      CO     NOX   Price  \\\n",
       "13312  24.093  1010.2  75.821  4.0023  25.762  550.25  1.2643  60.954  134.46   \n",
       "12627  20.450  1014.4  90.471  3.2106  20.085  549.94  2.6937  56.658  111.88   \n",
       "6393   20.262  1012.5  82.892  4.5325  25.221  549.62  1.9625  64.937  133.72   \n",
       "4990   26.862  1012.3  70.267  4.4266  25.965  549.96  1.5712  64.836  133.79   \n",
       "12462  19.409  1006.3  90.525  3.1241  19.688  550.01  2.2996  58.706  110.77   \n",
       "\n",
       "       Predicted Price       APE  \n",
       "13312       134.573898  0.084708  \n",
       "12627       112.686584  0.720937  \n",
       "6393        134.042786  0.241389  \n",
       "4990        134.094818  0.227833  \n",
       "12462       111.484459  0.644993  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing the absolute percent error\n",
    "APE=100*(abs(final_table['Price']-final_table['Predicted Price'])/final_table['Price'])\n",
    "print('The Accuracy for Test Data -- ANN model = ', 100-np.mean(APE))\n",
    "\n",
    "# adding absolute percent error to table\n",
    "final_table['APE']=APE\n",
    "final_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8209ea64",
   "metadata": {},
   "source": [
    "### Standardizing both Predictor & Response variable  - before train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0100ee6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12031, 8)\n",
      "(12031, 1)\n",
      "(3008, 8)\n",
      "(3008, 1)\n"
     ]
    }
   ],
   "source": [
    "### Sandardization of data ###\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    " \n",
    "# Storing the fit object for later reference\n",
    "x_scaler_fit = scaler_x.fit(x)\n",
    "y_scaler_fit = scaler_y.fit(y)\n",
    " \n",
    "# Generating the standardized values of X and y\n",
    "x = x_scaler_fit.transform(x)\n",
    "y = y_scaler_fit.transform(y)\n",
    " \n",
    "# Split the data into training and testing set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    " \n",
    "# Shape of Training and Test datasets\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "356d6707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722us/step - loss: 0.4862 - mae: 0.4413\n",
      "Epoch 2/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 0.0091 - mae: 0.0665\n",
      "Epoch 3/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 0.0049 - mae: 0.0490\n",
      "Epoch 4/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 0.0038 - mae: 0.0439\n",
      "Epoch 5/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 0.0035 - mae: 0.0424\n",
      "Epoch 6/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 0.0033 - mae: 0.0409\n",
      "Epoch 7/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 0.0032 - mae: 0.0403\n",
      "Epoch 8/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - loss: 0.0030 - mae: 0.0395\n",
      "Epoch 9/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0027 - mae: 0.0379\n",
      "Epoch 10/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 0.0030 - mae: 0.0392\n",
      "Epoch 11/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 0.0029 - mae: 0.0388\n",
      "Epoch 12/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.0027 - mae: 0.0379\n",
      "Epoch 13/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.0028 - mae: 0.0381\n",
      "Epoch 14/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.0028 - mae: 0.0381\n",
      "Epoch 15/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 0.0029 - mae: 0.0388\n",
      "Epoch 16/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 0.0029 - mae: 0.0390\n",
      "Epoch 17/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.0027 - mae: 0.0375\n",
      "Epoch 18/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0026 - mae: 0.0365\n",
      "Epoch 19/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.0026 - mae: 0.0369\n",
      "Epoch 20/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.0026 - mae: 0.0365\n",
      "Epoch 21/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 0.0024 - mae: 0.0358\n",
      "Epoch 22/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 0.0023 - mae: 0.0350\n",
      "Epoch 23/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 0.0027 - mae: 0.0373\n",
      "Epoch 24/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.0024 - mae: 0.0355\n",
      "Epoch 25/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.0026 - mae: 0.0364\n",
      "Epoch 26/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 0.0023 - mae: 0.0349\n",
      "Epoch 27/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 0.0024 - mae: 0.0354\n",
      "Epoch 28/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 0.0024 - mae: 0.0349\n",
      "Epoch 29/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 0.0023 - mae: 0.0353\n",
      "Epoch 30/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.0025 - mae: 0.0359\n",
      "Epoch 31/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.0022 - mae: 0.0344\n",
      "Epoch 32/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.0025 - mae: 0.0357\n",
      "Epoch 33/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 0.0025 - mae: 0.0355\n",
      "Epoch 34/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 0.0024 - mae: 0.0354\n",
      "Epoch 35/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 0.0023 - mae: 0.0351\n",
      "Epoch 36/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 0.0023 - mae: 0.0351\n",
      "Epoch 37/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.0023 - mae: 0.0349\n",
      "Epoch 38/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.0024 - mae: 0.0355\n",
      "Epoch 39/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 0.0024 - mae: 0.0354\n",
      "Epoch 40/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 0.0023 - mae: 0.0347\n",
      "Epoch 41/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 0.0021 - mae: 0.0340\n",
      "Epoch 42/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 0.0023 - mae: 0.0349\n",
      "Epoch 43/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 0.0024 - mae: 0.0347\n",
      "Epoch 44/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - loss: 0.0025 - mae: 0.0358\n",
      "Epoch 45/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 0.0022 - mae: 0.0346\n",
      "Epoch 46/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.0022 - mae: 0.0342\n",
      "Epoch 47/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.0023 - mae: 0.0343\n",
      "Epoch 48/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.0022 - mae: 0.0337\n",
      "Epoch 49/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 0.0023 - mae: 0.0344\n",
      "Epoch 50/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 0.0024 - mae: 0.0349\n",
      "Epoch 51/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 0.0020 - mae: 0.0331\n",
      "Epoch 52/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - loss: 0.0021 - mae: 0.0335\n",
      "Epoch 53/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 0.0025 - mae: 0.0349\n",
      "Epoch 54/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 0.0023 - mae: 0.0351\n",
      "Epoch 55/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 0.0023 - mae: 0.0337\n",
      "Epoch 56/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 0.0022 - mae: 0.0340\n",
      "Epoch 57/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 0.0022 - mae: 0.0337\n",
      "Epoch 58/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 0.0022 - mae: 0.0340\n",
      "Epoch 59/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 0.0021 - mae: 0.0330\n",
      "Epoch 60/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 0.0021 - mae: 0.0335\n",
      "Epoch 61/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 0.0021 - mae: 0.0337\n",
      "Epoch 62/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 0.0021 - mae: 0.0336\n",
      "Epoch 63/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 0.0021 - mae: 0.0331\n",
      "Epoch 64/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.0021 - mae: 0.0332\n",
      "Epoch 65/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.0021 - mae: 0.0334\n",
      "Epoch 66/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 0.0021 - mae: 0.0335\n",
      "Epoch 67/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 0.0021 - mae: 0.0329\n",
      "Epoch 68/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 0.0021 - mae: 0.0332\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896us/step - loss: 0.0021 - mae: 0.0330\n",
      "Epoch 70/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 0.0021 - mae: 0.0330\n",
      "Epoch 71/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 0.0020 - mae: 0.0327\n",
      "Epoch 72/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0021 - mae: 0.0336\n",
      "Epoch 73/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 0.0022 - mae: 0.0339\n",
      "Epoch 74/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 0.0022 - mae: 0.0333\n",
      "Epoch 75/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 0.0024 - mae: 0.0348\n",
      "Epoch 76/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 0.0021 - mae: 0.0330\n",
      "Epoch 77/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 0.0019 - mae: 0.0319\n",
      "Epoch 78/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 0.0021 - mae: 0.0331\n",
      "Epoch 79/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.0022 - mae: 0.0334\n",
      "Epoch 80/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.0022 - mae: 0.0333\n",
      "Epoch 81/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 0.0020 - mae: 0.0323\n",
      "Epoch 82/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 0.0020 - mae: 0.0331\n",
      "Epoch 83/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 0.0021 - mae: 0.0334\n",
      "Epoch 84/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 0.0021 - mae: 0.0331\n",
      "Epoch 85/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 0.0021 - mae: 0.0333\n",
      "Epoch 86/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 0.0020 - mae: 0.0329\n",
      "Epoch 87/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 0.0020 - mae: 0.0327\n",
      "Epoch 88/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 0.0021 - mae: 0.0325\n",
      "Epoch 89/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step - loss: 0.0021 - mae: 0.0328\n",
      "Epoch 90/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - loss: 0.0020 - mae: 0.0320\n",
      "Epoch 91/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 0.0020 - mae: 0.0324\n",
      "Epoch 92/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.0022 - mae: 0.0332\n",
      "Epoch 93/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 0.0019 - mae: 0.0318\n",
      "Epoch 94/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 0.0022 - mae: 0.0335\n",
      "Epoch 95/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 0.0019 - mae: 0.0319\n",
      "Epoch 96/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0021 - mae: 0.0332  \n",
      "Epoch 97/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 0.0019 - mae: 0.0312\n",
      "Epoch 98/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 0.0020 - mae: 0.0323\n",
      "Epoch 99/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 0.0020 - mae: 0.0325\n",
      "Epoch 100/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 0.0020 - mae: 0.0319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26e156da290>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we have continuous ouput, AF is not required in the o/p layer\n",
    "model = Sequential()\n",
    "model.add( Dense( units = 50 , activation = 'relu' , kernel_initializer = 'normal', input_dim = 8)) # input layer\n",
    "model.add( Dense( units = 20 , activation = 'tanh' , kernel_initializer = 'normal' )) # hidden layer\n",
    "model.add( Dense( units = 1  , kernel_initializer = 'normal' )) # o/p layer\n",
    "\n",
    "model.compile(optimizer ='adam', loss = 'mean_squared_error', metrics=['mae'])\n",
    "model.fit(x_train, y_train , batch_size=50, epochs=100,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a9c0cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step\n",
      "batch_size: 5 - epochs: 5 Accuracy: 49.50238034686596\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "batch_size: 5 - epochs: 10 Accuracy: 70.13515544128234\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step\n",
      "batch_size: 5 - epochs: 50 Accuracy: 44.037366065299686\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step\n",
      "batch_size: 5 - epochs: 100 Accuracy: 52.06928358759072\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step\n",
      "batch_size: 10 - epochs: 5 Accuracy: 8.470804348561671\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step\n",
      "batch_size: 10 - epochs: 10 Accuracy: 45.35094609984004\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step\n",
      "batch_size: 10 - epochs: 50 Accuracy: 50.759951422236014\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step\n",
      "batch_size: 10 - epochs: 100 Accuracy: 71.78953000291179\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "batch_size: 15 - epochs: 5 Accuracy: 23.688182154025142\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step\n",
      "batch_size: 15 - epochs: 10 Accuracy: 33.58109575120859\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "batch_size: 15 - epochs: 50 Accuracy: 52.10754957156145\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 838us/step\n",
      "batch_size: 15 - epochs: 100 Accuracy: 49.512008084127096\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "batch_size: 20 - epochs: 5 Accuracy: 41.83076173385656\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step\n",
      "batch_size: 20 - epochs: 10 Accuracy: 39.30219584696658\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "batch_size: 20 - epochs: 50 Accuracy: 57.35062099665413\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step\n",
      "batch_size: 20 - epochs: 100 Accuracy: 60.81200447106774\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batchsize</th>\n",
       "      <th>epochs</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>49.502380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>70.135155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>44.037366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>52.069284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>8.470804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>45.350946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>50.759951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>71.789530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>23.688182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>33.581096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>50</td>\n",
       "      <td>52.107550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>49.512008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>41.830762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>39.302196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>57.350621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>60.812004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batchsize  epochs   Accuracy\n",
       "0          5       5  49.502380\n",
       "0          5      10  70.135155\n",
       "0          5      50  44.037366\n",
       "0          5     100  52.069284\n",
       "0         10       5   8.470804\n",
       "0         10      10  45.350946\n",
       "0         10      50  50.759951\n",
       "0         10     100  71.789530\n",
       "0         15       5  23.688182\n",
       "0         15      10  33.581096\n",
       "0         15      50  52.107550\n",
       "0         15     100  49.512008\n",
       "0         20       5  41.830762\n",
       "0         20      10  39.302196\n",
       "0         20      50  57.350621\n",
       "0         20     100  60.812004"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def toFindBestParams(x_train, y_train, x_test, y_test):\n",
    "        \n",
    "    #defining list of hyperparameters\n",
    "    batch_size_list = [5 , 10 , 15 , 20]\n",
    "    epoch_list      = [5 , 10 , 50 , 100]\n",
    "    \n",
    "    bestParamTable = pd.DataFrame()\n",
    "    \n",
    "    for batch_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "                        \n",
    "            # create ANN model\n",
    "            model = Sequential()\n",
    "            # Defining the first layer of the model\n",
    "            model.add(Dense(units=50, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "            \n",
    "            # Defining the Second layer of the model\n",
    "            model.add(Dense(units=20, kernel_initializer='normal', activation='tanh'))\n",
    " \n",
    "            # The output neuron is 1 as o/p is continuous\n",
    "            # No AF needed coz continuous output\n",
    "            model.add(Dense(1, kernel_initializer='normal'))\n",
    " \n",
    "            # Compiling the model\n",
    "            model.compile(optimizer ='adam', loss = 'mean_squared_error')\n",
    "            \n",
    "            # Fitting the ANN to the Training set\n",
    "            model.fit(x_train, y_train , batch_size=batch_trial, epochs=epochs_trial,  verbose=0)\n",
    "                        \n",
    "            MAPE = np.mean(100 * (np.abs(y_test-model.predict(x_test))/y_test))\n",
    "            \n",
    "            bestParamTable=bestParamTable.append(pd.DataFrame(data=[[batch_trial, epochs_trial, 100-MAPE]],\n",
    "                                                        columns=['batchsize','epochs','Accuracy'] ))\n",
    "            \n",
    "            #printing the results of the current iteration\n",
    "            print('batch_size:', batch_trial,'-', 'epochs:',epochs_trial, 'Accuracy:',100-MAPE)\n",
    "            \n",
    "    return bestParamTable\n",
    "\n",
    "# Calling the function\n",
    "finalParamTable = toFindBestParams(x_train, y_train, x_test, y_test)\n",
    "#print(finalParamTable['Accuracy'].max())\n",
    "#print(finalParamTable[finalParamTable['Accuracy'].max()])\n",
    "finalParamTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f82de20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index          0.00000\n",
       "batchsize     10.00000\n",
       "epochs       100.00000\n",
       "Accuracy      71.78953\n",
       "Name: 7, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting corresponding row values of the maximum value of 'Accuracy' column\n",
    "finalParamTable = finalParamTable.reset_index()\n",
    "#print(finalParamTable)\n",
    "#print(finalParamTable['Accuracy'].idxmax())\n",
    "finalParamTable.iloc[finalParamTable['Accuracy'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "632223fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26e19d49d50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model to best params\n",
    "model.compile(optimizer ='adam', loss = 'mean_squared_error')\n",
    "model.fit(x_train,y_train, batch_size=10 , epochs = 50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d2aca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "(3008, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>Price</th>\n",
       "      <th>Predicted Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.093</td>\n",
       "      <td>1010.2</td>\n",
       "      <td>75.821</td>\n",
       "      <td>4.0023</td>\n",
       "      <td>25.762</td>\n",
       "      <td>550.25</td>\n",
       "      <td>1.26430</td>\n",
       "      <td>60.954</td>\n",
       "      <td>134.46</td>\n",
       "      <td>134.465591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.450</td>\n",
       "      <td>1014.4</td>\n",
       "      <td>90.471</td>\n",
       "      <td>3.2106</td>\n",
       "      <td>20.085</td>\n",
       "      <td>549.94</td>\n",
       "      <td>2.69370</td>\n",
       "      <td>56.658</td>\n",
       "      <td>111.88</td>\n",
       "      <td>112.635361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.262</td>\n",
       "      <td>1012.5</td>\n",
       "      <td>82.892</td>\n",
       "      <td>4.5325</td>\n",
       "      <td>25.221</td>\n",
       "      <td>549.62</td>\n",
       "      <td>1.96250</td>\n",
       "      <td>64.937</td>\n",
       "      <td>133.72</td>\n",
       "      <td>133.805389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.862</td>\n",
       "      <td>1012.3</td>\n",
       "      <td>70.267</td>\n",
       "      <td>4.4266</td>\n",
       "      <td>25.965</td>\n",
       "      <td>549.96</td>\n",
       "      <td>1.57120</td>\n",
       "      <td>64.836</td>\n",
       "      <td>133.79</td>\n",
       "      <td>133.821732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.409</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>90.525</td>\n",
       "      <td>3.1241</td>\n",
       "      <td>19.688</td>\n",
       "      <td>550.01</td>\n",
       "      <td>2.29960</td>\n",
       "      <td>58.706</td>\n",
       "      <td>110.77</td>\n",
       "      <td>111.272018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3003</th>\n",
       "      <td>21.692</td>\n",
       "      <td>1006.2</td>\n",
       "      <td>80.869</td>\n",
       "      <td>3.3145</td>\n",
       "      <td>21.936</td>\n",
       "      <td>549.87</td>\n",
       "      <td>1.42310</td>\n",
       "      <td>59.948</td>\n",
       "      <td>119.25</td>\n",
       "      <td>119.849426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>27.200</td>\n",
       "      <td>1012.1</td>\n",
       "      <td>58.947</td>\n",
       "      <td>4.0224</td>\n",
       "      <td>25.864</td>\n",
       "      <td>550.41</td>\n",
       "      <td>0.78249</td>\n",
       "      <td>64.662</td>\n",
       "      <td>133.74</td>\n",
       "      <td>133.920074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005</th>\n",
       "      <td>29.073</td>\n",
       "      <td>1008.3</td>\n",
       "      <td>64.511</td>\n",
       "      <td>4.7190</td>\n",
       "      <td>29.741</td>\n",
       "      <td>541.25</td>\n",
       "      <td>1.64630</td>\n",
       "      <td>59.024</td>\n",
       "      <td>146.31</td>\n",
       "      <td>146.626572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>23.404</td>\n",
       "      <td>1011.8</td>\n",
       "      <td>71.443</td>\n",
       "      <td>4.8017</td>\n",
       "      <td>30.052</td>\n",
       "      <td>538.12</td>\n",
       "      <td>0.84607</td>\n",
       "      <td>67.295</td>\n",
       "      <td>150.07</td>\n",
       "      <td>149.503662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>20.168</td>\n",
       "      <td>1008.2</td>\n",
       "      <td>82.547</td>\n",
       "      <td>3.0775</td>\n",
       "      <td>19.808</td>\n",
       "      <td>549.92</td>\n",
       "      <td>5.75540</td>\n",
       "      <td>53.265</td>\n",
       "      <td>111.77</td>\n",
       "      <td>110.691147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3008 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0       1       2       3       4       5        6       7   Price  \\\n",
       "0     24.093  1010.2  75.821  4.0023  25.762  550.25  1.26430  60.954  134.46   \n",
       "1     20.450  1014.4  90.471  3.2106  20.085  549.94  2.69370  56.658  111.88   \n",
       "2     20.262  1012.5  82.892  4.5325  25.221  549.62  1.96250  64.937  133.72   \n",
       "3     26.862  1012.3  70.267  4.4266  25.965  549.96  1.57120  64.836  133.79   \n",
       "4     19.409  1006.3  90.525  3.1241  19.688  550.01  2.29960  58.706  110.77   \n",
       "...      ...     ...     ...     ...     ...     ...      ...     ...     ...   \n",
       "3003  21.692  1006.2  80.869  3.3145  21.936  549.87  1.42310  59.948  119.25   \n",
       "3004  27.200  1012.1  58.947  4.0224  25.864  550.41  0.78249  64.662  133.74   \n",
       "3005  29.073  1008.3  64.511  4.7190  29.741  541.25  1.64630  59.024  146.31   \n",
       "3006  23.404  1011.8  71.443  4.8017  30.052  538.12  0.84607  67.295  150.07   \n",
       "3007  20.168  1008.2  82.547  3.0775  19.808  549.92  5.75540  53.265  111.77   \n",
       "\n",
       "      Predicted Price  \n",
       "0          134.465591  \n",
       "1          112.635361  \n",
       "2          133.805389  \n",
       "3          133.821732  \n",
       "4          111.272018  \n",
       "...               ...  \n",
       "3003       119.849426  \n",
       "3004       133.920074  \n",
       "3005       146.626572  \n",
       "3006       149.503662  \n",
       "3007       110.691147  \n",
       "\n",
       "[3008 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating predictions for test data\n",
    "y_predict_test = model.predict(x_test) \n",
    "\n",
    "# scaling back test data to original data\n",
    "y_test_original = y_scaler_fit.inverse_transform(y_test)\n",
    "\n",
    "# Scaling the predicted Price data back to original price scale\n",
    "y_predict_test=y_scaler_fit.inverse_transform(y_predict_test)\n",
    "\n",
    "# scaling the test input data back to original\n",
    "x_test_original = x_scaler_fit.inverse_transform(x_test)\n",
    "\n",
    "# creating table with descaled test price & descaled predicted price for test\n",
    "final_table_1 = pd.DataFrame(x_test_original)\n",
    "final_table_1['Price'] = y_test_original\n",
    "final_table_1['Predicted Price'] = y_predict_test\n",
    "print(final_table_1.shape)\n",
    "final_table_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58d9959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy for Test Data -- ANN model =  99.63660261537171\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>Price</th>\n",
       "      <th>Predicted Price</th>\n",
       "      <th>APE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0930</td>\n",
       "      <td>1010.2</td>\n",
       "      <td>75.821</td>\n",
       "      <td>4.0023</td>\n",
       "      <td>25.762</td>\n",
       "      <td>550.25</td>\n",
       "      <td>1.26430</td>\n",
       "      <td>60.954</td>\n",
       "      <td>134.46</td>\n",
       "      <td>134.465591</td>\n",
       "      <td>0.004158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.4500</td>\n",
       "      <td>1014.4</td>\n",
       "      <td>90.471</td>\n",
       "      <td>3.2106</td>\n",
       "      <td>20.085</td>\n",
       "      <td>549.94</td>\n",
       "      <td>2.69370</td>\n",
       "      <td>56.658</td>\n",
       "      <td>111.88</td>\n",
       "      <td>112.635361</td>\n",
       "      <td>0.675153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.2620</td>\n",
       "      <td>1012.5</td>\n",
       "      <td>82.892</td>\n",
       "      <td>4.5325</td>\n",
       "      <td>25.221</td>\n",
       "      <td>549.62</td>\n",
       "      <td>1.96250</td>\n",
       "      <td>64.937</td>\n",
       "      <td>133.72</td>\n",
       "      <td>133.805389</td>\n",
       "      <td>0.063857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.8620</td>\n",
       "      <td>1012.3</td>\n",
       "      <td>70.267</td>\n",
       "      <td>4.4266</td>\n",
       "      <td>25.965</td>\n",
       "      <td>549.96</td>\n",
       "      <td>1.57120</td>\n",
       "      <td>64.836</td>\n",
       "      <td>133.79</td>\n",
       "      <td>133.821732</td>\n",
       "      <td>0.023717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.4090</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>90.525</td>\n",
       "      <td>3.1241</td>\n",
       "      <td>19.688</td>\n",
       "      <td>550.01</td>\n",
       "      <td>2.29960</td>\n",
       "      <td>58.706</td>\n",
       "      <td>110.77</td>\n",
       "      <td>111.272018</td>\n",
       "      <td>0.453208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.2158</td>\n",
       "      <td>1013.1</td>\n",
       "      <td>88.695</td>\n",
       "      <td>3.3709</td>\n",
       "      <td>19.387</td>\n",
       "      <td>550.17</td>\n",
       "      <td>4.27640</td>\n",
       "      <td>99.759</td>\n",
       "      <td>113.32</td>\n",
       "      <td>114.539749</td>\n",
       "      <td>1.076376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.8520</td>\n",
       "      <td>1002.2</td>\n",
       "      <td>84.186</td>\n",
       "      <td>3.7871</td>\n",
       "      <td>25.392</td>\n",
       "      <td>550.11</td>\n",
       "      <td>0.83578</td>\n",
       "      <td>59.426</td>\n",
       "      <td>133.77</td>\n",
       "      <td>133.639267</td>\n",
       "      <td>0.097730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.3950</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>95.381</td>\n",
       "      <td>4.2837</td>\n",
       "      <td>23.225</td>\n",
       "      <td>549.60</td>\n",
       "      <td>2.01980</td>\n",
       "      <td>75.260</td>\n",
       "      <td>128.98</td>\n",
       "      <td>129.237274</td>\n",
       "      <td>0.199468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12.3590</td>\n",
       "      <td>1022.7</td>\n",
       "      <td>82.295</td>\n",
       "      <td>5.1559</td>\n",
       "      <td>32.518</td>\n",
       "      <td>528.98</td>\n",
       "      <td>0.87760</td>\n",
       "      <td>66.416</td>\n",
       "      <td>159.42</td>\n",
       "      <td>160.064194</td>\n",
       "      <td>0.404086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.6376</td>\n",
       "      <td>1001.6</td>\n",
       "      <td>98.271</td>\n",
       "      <td>5.9309</td>\n",
       "      <td>32.105</td>\n",
       "      <td>530.69</td>\n",
       "      <td>10.75000</td>\n",
       "      <td>102.130</td>\n",
       "      <td>161.86</td>\n",
       "      <td>161.118759</td>\n",
       "      <td>0.457952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5         6        7   Price  \\\n",
       "0  24.0930  1010.2  75.821  4.0023  25.762  550.25   1.26430   60.954  134.46   \n",
       "1  20.4500  1014.4  90.471  3.2106  20.085  549.94   2.69370   56.658  111.88   \n",
       "2  20.2620  1012.5  82.892  4.5325  25.221  549.62   1.96250   64.937  133.72   \n",
       "3  26.8620  1012.3  70.267  4.4266  25.965  549.96   1.57120   64.836  133.79   \n",
       "4  19.4090  1006.3  90.525  3.1241  19.688  550.01   2.29960   58.706  110.77   \n",
       "5   2.2158  1013.1  88.695  3.3709  19.387  550.17   4.27640   99.759  113.32   \n",
       "6  23.8520  1002.2  84.186  3.7871  25.392  550.11   0.83578   59.426  133.77   \n",
       "7  12.3950  1019.5  95.381  4.2837  23.225  549.60   2.01980   75.260  128.98   \n",
       "8  12.3590  1022.7  82.295  5.1559  32.518  528.98   0.87760   66.416  159.42   \n",
       "9   8.6376  1001.6  98.271  5.9309  32.105  530.69  10.75000  102.130  161.86   \n",
       "\n",
       "   Predicted Price       APE  \n",
       "0       134.465591  0.004158  \n",
       "1       112.635361  0.675153  \n",
       "2       133.805389  0.063857  \n",
       "3       133.821732  0.023717  \n",
       "4       111.272018  0.453208  \n",
       "5       114.539749  1.076376  \n",
       "6       133.639267  0.097730  \n",
       "7       129.237274  0.199468  \n",
       "8       160.064194  0.404086  \n",
       "9       161.118759  0.457952  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing the absolute percent error\n",
    "APE_1 = 100*(abs(final_table_1['Price']-final_table_1['Predicted Price'])/final_table_1['Price'])\n",
    "print('The Accuracy for Test Data -- ANN model = ', 100-np.mean(APE_1))\n",
    "\n",
    "# adding absolute percent error to table\n",
    "final_table_1['APE'] = APE_1\n",
    "final_table_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3bd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
